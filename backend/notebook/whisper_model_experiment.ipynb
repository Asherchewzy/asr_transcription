{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d14df0",
   "metadata": {},
   "source": [
    "# Finding the best params\n",
    "## notebook seeks to find what params work and note down some discoveries along the way "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17438c06",
   "metadata": {},
   "source": [
    "### Whisper Tiny (Hugging Face)\n",
    "\n",
    "- **Model**: `openai/whisper-tiny` (pre-trained, multilingual)\n",
    "- **Tasks**: Automatic speech recognition (ASR) and speech translation\n",
    "- **Architecture**: Encoder–decoder Transformer (sequence-to-sequence)\n",
    "- **Input pipeline**:\n",
    "  - Audio resampled to **16 kHz**\n",
    "  - 80-channel log-magnitude **Mel spectrogram**\n",
    "  - 25 ms window, 10 ms stride\n",
    "  - Spectrogram normalized to **[-1, 1]** with near-zero mean\n",
    "- **References**:\n",
    "  - Hugging Face: https://huggingface.co/openai/whisper-tiny\n",
    "  - Paper: https://arxiv.org/abs/2212.04356\n",
    "  - GitHub: https://github.com/openai/whisper\n",
    "  - Overview: https://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ba39e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### try huggingface pipeline\n",
    "Hugging Face pipeline is a high-level wrapper\n",
    "https://huggingface.co/docs/transformers/en/model_doc/whisper\n",
    "https://huggingface.co/docs/transformers/en/model_doc/whisper?usage=Pipeline\n",
    "\n",
    "\n",
    "https://huggingface.co/openai/whisper-tiny\n",
    "long-Form Transcription\n",
    "The Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking algorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers pipeline method. Chunking is enabled by setting chunk_length_s=30 when instantiating the pipeline. With chunking enabled, the pipeline can be run with batched inference. It can also be extended to predict sequence level timestamps by passing return_timestamps=True:\n",
    "\n",
    "https://huggingface.co/openai/whisper-large-v3\n",
    "\"By default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the chunk_length_s parameter to the pipeline. For large-v3, a chunk length of 30-seconds is optimal.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b42b8c",
   "metadata": {},
   "source": [
    "### ASR \n",
    "WER for eng seems to be a good measure of success. \n",
    "https://www.youtube.com/watch?v=TksaY_FDgnk\n",
    "https://huggingface.co/docs/transformers/en/tasks/asr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef32d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per file: {'Sample 1.mp3': {'duration_sec': 13.12, 'bitrate_kbps': 124}, 'Sample 2.mp3': {'duration_sec': 11.050667, 'bitrate_kbps': 124}, 'Sample 3.mp3': {'duration_sec': 12.842667, 'bitrate_kbps': 126}}\n",
      "Average duration (sec): 12.337778\n",
      "Average bitrate (kbps): 124.66666666666667\n"
     ]
    }
   ],
   "source": [
    "import json, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "audio_files = [\"../../Sample 1.mp3\", \"../../Sample 2.mp3\", \"../../Sample 3.mp3\"]\n",
    "\n",
    "def ffprobe_info(path: str) -> dict:\n",
    "    cmd = [\n",
    "        \"ffprobe\", \"-v\", \"error\",\n",
    "        \"-show_entries\", \"format=duration:stream=bit_rate\",\n",
    "        \"-of\", \"json\",\n",
    "        path,\n",
    "    ]\n",
    "    data = json.loads(subprocess.check_output(cmd))\n",
    "    duration = float(data[\"format\"][\"duration\"])\n",
    "    # grab the first audio stream’s bitrate (bits/sec) and convert to kbps\n",
    "    streams = data.get(\"streams\", [])\n",
    "    bit_rate_bps = int(streams[0][\"bit_rate\"]) if streams else int(data[\"format\"][\"bit_rate\"])\n",
    "    return {\"duration_sec\": duration, \"bitrate_kbps\": bit_rate_bps // 1000}\n",
    "\n",
    "info = {Path(f).name: ffprobe_info(f) for f in audio_files}\n",
    "\n",
    "print(\"Per file:\", info)\n",
    "avg_duration = sum(v[\"duration_sec\"] for v in info.values()) / len(info)\n",
    "avg_kbps = sum(v[\"bitrate_kbps\"] for v in info.values()) / len(info)\n",
    "print(\"Average duration (sec):\", avg_duration)\n",
    "print(\"Average bitrate (kbps):\", avg_kbps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "357aa579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0427807486631"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_size_mb = 15\n",
    "minutes = (max_size_mb * 8_000_000) / (avg_kbps * 1000 * 60) \n",
    "minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d48e34",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6f3b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# set up device\n",
    "import os, torch\n",
    "\n",
    "# helps on some Macs if an op isn't supported on MPS\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "device = pick_device()\n",
    "dtype = torch.float16 if device.startswith(\"cuda\") else torch.float32\n",
    "print(\"device:\", device, \"dtype:\", dtype)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7219a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geraldinelim/Documents/Documents - Mac/asher_local/htx/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6345ff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 587/587 [00:02<00:00, 230.62it/s, Materializing param=model.encoder.layers.31.self_attn_layer_norm.weight] \n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What should I have for lunch? There's only young tofu, western, Japanese, economic rice stalls here. I am sick of the choices here.\n"
     ]
    }
   ],
   "source": [
    "whisper_large_v3_turbo = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3-turbo\", device=device, dtype=dtype)\n",
    "result = whisper_large_v3_turbo(\"../../Sample 3.mp3\", chunk_length_s=30, stride_length_s=(4, 4))\n",
    "print(result['text'].strip())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0db17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 167/167 [00:00<00:00, 2400.22it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]  \n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What should I have for lunch? There's only young tofu, Western, Japanese, economic rice stalls here. I'm sick of the choices here.\n"
     ]
    }
   ],
   "source": [
    "whisper_tiny = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny\", device=device, dtype=dtype)\n",
    "result = whisper_tiny(\"../../Sample 3.mp3\", chunk_length_s=30, stride_length_s=(4, 4))\n",
    "print(result['text'].strip())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975eae01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Ethan. I was asked to come here by 11. Now it is already 3 p.m. They did not even serve me any food or drinks. Terrible.\n"
     ]
    }
   ],
   "source": [
    "result = whisper_tiny(\"../../Sample 1.mp3\", chunk_length_s=30, stride_length_s=(4, 4))\n",
    "print(result['text'].strip())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c346f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help me. I can't find my parents. They told me to wait for them, but I saw this pretty butterfly and followed it. Now I am lost.\n"
     ]
    }
   ],
   "source": [
    "result = whisper_tiny(\"../../Sample 2.mp3\", chunk_length_s=30, stride_length_s=(4, 4))\n",
    "print(result['text'].strip())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5c380",
   "metadata": {},
   "source": [
    "## questions\n",
    "- will shorter chunk_length_s be better? \n",
    "- what left and right stride to use?\n",
    "- should we try forcing the language and task \n",
    "\n",
    "initial thoughts was to perform a customized grid search to quickly find the best chunk length, left/right stride and weather try if forcing language/task result in better outcome using metrics like WER. But out of the 3 audio, there is only 2-3 word error all due to \"yong tou fu\" not \"young tofu\". however, it is close enough since tofu is in the oxford dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d7734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend (.venv)",
   "language": "python",
   "name": "htx-backend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
