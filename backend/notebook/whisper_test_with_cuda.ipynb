{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ran on T4 gpu, cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T11:14:29.284713Z",
     "iopub.status.busy": "2026-01-28T11:14:29.284501Z",
     "iopub.status.idle": "2026-01-28T11:14:35.927325Z",
     "shell.execute_reply": "2026-01-28T11:14:35.926520Z",
     "shell.execute_reply.started": "2026-01-28T11:14:29.284691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# set up device\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# helps on some Macs if an op isn't supported on MPS\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T11:14:35.929414Z",
     "iopub.status.busy": "2026-01-28T11:14:35.929040Z",
     "iopub.status.idle": "2026-01-28T11:14:36.030138Z",
     "shell.execute_reply": "2026-01-28T11:14:36.029412Z",
     "shell.execute_reply.started": "2026-01-28T11:14:35.929363Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0 dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "device = pick_device()\n",
    "dtype = torch.float16 if device.startswith(\"cuda\") else torch.float32\n",
    "print(\"device:\", device, \"dtype:\", dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T11:15:24.974086Z",
     "iopub.status.busy": "2026-01-28T11:15:24.973209Z",
     "iopub.status.idle": "2026-01-28T11:15:59.903172Z",
     "shell.execute_reply": "2026-01-28T11:15:59.902558Z",
     "shell.execute_reply.started": "2026-01-28T11:15:24.974044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 11:15:36.773776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769598937.159462      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769598937.277171      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769598938.291703      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769598938.291748      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769598938.291752      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769598938.291754      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T11:16:10.502092Z",
     "iopub.status.busy": "2026-01-28T11:16:10.501126Z",
     "iopub.status.idle": "2026-01-28T11:16:20.277566Z",
     "shell.execute_reply": "2026-01-28T11:16:20.276609Z",
     "shell.execute_reply.started": "2026-01-28T11:16:10.502058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a288a58d72024c7faf993154d535bc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5755134330d94f5dbe408d46b5e1b9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb51ad1f6f74e2eaa982e18aac3193f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19ccf5c55e241798335c74a14af77e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d50aef72dc4a1db7b9c943ce567804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609e3d3623684f6fb444fe423c819a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6d553a26054881bf434e83a5d8c309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01965879abf4bb9890568c26de67219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8678f0f3730841d69f3c5bf506bf0b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14775d18ebd48ed8404d3c146dca5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d070b95f9244818b8efea524947604d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What should I have for lunch? There's only young tofu, Western, Japanese, economic rice stalls here. I'm sick of the choices here.\n"
     ]
    }
   ],
   "source": [
    "whisper_tiny = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-tiny\",\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "result = whisper_tiny(\n",
    "    \"/kaggle/input/audio-file/Sample 3.mp3\", chunk_length_s=30, stride_length_s=(4, 4)\n",
    ")\n",
    "print(result[\"text\"].strip())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9359094,
     "sourceId": 14650667,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
