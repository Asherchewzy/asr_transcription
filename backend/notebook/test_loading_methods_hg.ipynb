{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40bc0615",
   "metadata": {},
   "source": [
    "# Test ground\n",
    "## notebook to test the basic loading and running of model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7d921",
   "metadata": {},
   "source": [
    "### Try Example on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install torchcodec datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba756f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geraldinelim/Documents/Documents - Mac/asher_local/htx/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 167/167 [00:00<00:00, 2244.05it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88df33b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load dummy dataset and read audio files\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"]\n",
    "input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n",
    "print(sample[\"sampling_rate\"], input_features.shape)\n",
    "\n",
    "# generate token ids\n",
    "predicted_ids = model.generate(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d85393e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\n",
      "[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\n"
     ]
    }
   ],
   "source": [
    "# decode token ids to text\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "print(transcription)\n",
    "\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110463f2",
   "metadata": {},
   "source": [
    "#### Notes from the page\n",
    "- `WhisperProcessor` handles preprocessing (audio → log-Mel spectrogram) and postprocessing (tokens → text).\n",
    "- Without forcing, Whisper auto-detects language and task.\n",
    "- Force English transcription:\n",
    "  ```py\n",
    "  model.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(\n",
    "      language=\"english\", task=\"transcribe\"\n",
    "  )\n",
    "\n",
    "<details> <summary><strong>Warnings to note</strong></summary>\n",
    "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
    "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
    "16000 torch.Size([1, 80, 3000])\n",
    "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
    "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83298820",
   "metadata": {},
   "source": [
    "### Try huggingface pipeline\n",
    "Hugging Face pipeline is a high-level wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aacbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# set up device\n",
    "import os, torch\n",
    "\n",
    "# helps on some Macs if an op isn't supported on MPS\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "device = pick_device()\n",
    "\n",
    "dtype = torch.float16 if device.startswith(\"cuda\") else torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4af7070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps dtype: torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 167/167 [00:00<00:00, 2981.60it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline                                                                                                                                                                               \n",
    "\n",
    "print(\"device:\", device, \"dtype:\", dtype)                                                                                                                                                                         \n",
    "whisper = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny\", device=device, torch_dtype=dtype)                                                                                                                                                                                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aafe970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    }
   ],
   "source": [
    "result = whisper(\"../../Sample 3.mp3\", chunk_length_s=30, stride_length_s = (4, 2))                                                                                                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2eb264f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" What should I have for lunch? There's only young tofu, Western, Japanese, economic rice stalls here. I'm sick of the choices here.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "250c544a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What should I have for lunch? There's only young tofu, Western, Japanese, economic rice stalls here. I'm sick of the choices here.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437a460",
   "metadata": {},
   "source": [
    "## questions\n",
    "- will shorter chunk_length_s be better? \n",
    "- what left and right stride to use?\n",
    "- should we try forcing the language and task "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend (.venv)",
   "language": "python",
   "name": "htx-backend"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
